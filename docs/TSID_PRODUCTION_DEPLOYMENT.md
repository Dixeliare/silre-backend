# TSID Production Deployment - Auto-Restart & Self-Healing

## ğŸ¯ Váº¥n Ä‘á»

**Fail-Fast lÃ  intentional** Ä‘á»ƒ prevent data corruption, nhÆ°ng trong production cáº§n cÃ³ **auto-restart mechanism** Ä‘á»ƒ Ä‘áº£m báº£o high availability.

---

## âœ… Giáº£i phÃ¡p: Auto-Restart + Self-Healing

### 1. **Fail-Fast â†’ Auto-Restart â†’ Self-Healing**

```
Timeline:
00:00 - Instance A: Node ID 0, Redis key exists
04:30 - Redis restart â†’ Key bá»‹ máº¥t
04:31 - Instance A: refreshTsidLock() detects lock lost
        â†’ FAIL-FAST: System.exit(1)
        â†’ App crashes
04:32 - Auto-restart mechanism (systemd/Docker/K8s) detects crash
        â†’ Restart instance A
04:33 - Instance A: allocateNodeId() runs
        â†’ Tries Node ID 0 â†’ SETNX â†’ Success (key was lost)
        â†’ Re-acquires Node ID 0
        â†’ âœ… Self-healing complete!
```

**Káº¿t quáº£:** Instance tá»± Ä‘á»™ng restart vÃ  re-acquire Node ID â†’ **Zero downtime** (náº¿u cÃ³ multiple instances)

---

## ğŸš€ Production Deployment Options

### Option 1: **Docker with Restart Policy** (Recommended for Docker)

```yaml
# docker-compose.yml
version: '3.8'
services:
  backend:
    image: silre-backend:latest
    restart: always  # Auto-restart on crash
    environment:
      - SPRING_PROFILES_ACTIVE=prod
    depends_on:
      - redis
      - postgres
  
  redis:
    image: redis:7-alpine
    restart: always
    volumes:
      - redis-data:/data
  
  postgres:
    image: postgres:15
    restart: always
    volumes:
      - postgres-data:/var/lib/postgresql/data
```

**Docker restart policies:**
- `always`: Restart always (even on manual stop)
- `unless-stopped`: Restart unless manually stopped
- `on-failure`: Restart only on failure (exit code != 0)

**Command:**
```bash
docker run -d --restart=always silre-backend:latest
```

---

### Option 2: **systemd Service** (Recommended for VPS/Bare Metal)

```ini
# /etc/systemd/system/silre-backend.service
[Unit]
Description=Silre Backend Application
After=network.target postgresql.service redis.service

[Service]
Type=simple
User=appuser
WorkingDirectory=/opt/silre-backend
ExecStart=/usr/bin/java -jar /opt/silre-backend/silre-backend.jar
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

# Auto-restart on failure
Restart=on-failure
RestartSec=10

# Resource limits
LimitNOFILE=65536
MemoryLimit=2G

[Install]
WantedBy=multi-user.target
```

**systemd restart policies:**
- `always`: Restart always
- `on-failure`: Restart only on failure (exit code != 0) âœ… **Recommended**
- `on-abnormal`: Restart on abnormal termination
- `on-watchdog`: Restart on watchdog timeout

**Commands:**
```bash
# Enable and start service
sudo systemctl enable silre-backend
sudo systemctl start silre-backend

# Check status
sudo systemctl status silre-backend

# View logs
sudo journalctl -u silre-backend -f
```

---

### Option 3: **Kubernetes Deployment** (Recommended for K8s)

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: silre-backend
spec:
  replicas: 3  # Multiple instances for high availability
  selector:
    matchLabels:
      app: silre-backend
  template:
    metadata:
      labels:
        app: silre-backend
    spec:
      containers:
      - name: backend
        image: silre-backend:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "prod"
        # Auto-restart on crash
        restartPolicy: Always
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        
        # Resource limits
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
```

**Kubernetes automatically:**
- âœ… Restarts crashed containers
- âœ… Maintains desired replica count
- âœ… Distributes traffic across healthy instances
- âœ… Handles rolling updates

---

## ğŸ”„ Self-Healing Flow

### Scenario: Redis Restart â†’ Lock Lost â†’ Auto-Restart

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Instance Running                                    â”‚
â”‚ Instance A: Node ID 0, Redis key = "instance:A:pid:time"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Redis Restart                                        â”‚
â”‚ Redis: All keys lost (no persistence)                        â”‚
â”‚ Instance A: Still running with Node ID 0 in memory          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Refresh Detects Lock Lost (within 12 hours)         â”‚
â”‚ refreshTsidLock() â†’ currentValue == null                     â”‚
â”‚ â†’ System.exit(1) â†’ FAIL-FAST                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Auto-Restart Mechanism                              â”‚
â”‚ systemd/Docker/K8s detects exit code != 0                  â”‚
â”‚ â†’ Restart instance A (within 10 seconds)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Self-Healing                                        â”‚
â”‚ Instance A: allocateNodeId() runs                          â”‚
â”‚ â†’ Tries Node ID 0 â†’ SETNX â†’ Success (key was lost)        â”‚
â”‚ â†’ Re-acquires Node ID 0                                     â”‚
â”‚ â†’ âœ… Back to normal operation!                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total downtime:** ~10-30 seconds (restart time)

---

## ğŸ“Š Production Best Practices

### 1. **Multiple Instances** (High Availability)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Instance A  â”‚     â”‚ Instance B  â”‚     â”‚ Instance C  â”‚
â”‚ Node ID: 0  â”‚     â”‚ Node ID: 1  â”‚     â”‚ Node ID: 2  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Load Balancerâ”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Zero downtime during restart (other instances handle traffic)
- âœ… Load distribution
- âœ… Fault tolerance

---

### 2. **Monitoring & Alerting**

```yaml
# Prometheus alert rules
groups:
- name: tsid_lock_alerts
  rules:
  - alert: TSIDLockLost
    expr: increase(tsid_lock_lost_total[5m]) > 0
    annotations:
      summary: "TSID lock lost - instance restarted"
      description: "Instance {{ $labels.instance }} lost TSID lock and restarted"
  
  - alert: TSIDLockStolen
    expr: increase(tsid_lock_stolen_total[5m]) > 0
    annotations:
      summary: "TSID lock stolen - potential ID collision prevented"
      description: "Instance {{ $labels.instance }} detected lock theft"
```

**Monitoring metrics:**
- `tsid_lock_lost_total`: Counter of lock loss events
- `tsid_lock_stolen_total`: Counter of lock theft events
- `tsid_node_id`: Current Node ID (gauge)
- `tsid_lock_ttl_seconds`: Remaining TTL (gauge)

---

### 3. **Redis Persistence** (Optional but Recommended)

```conf
# redis.conf
# Enable AOF (Append Only File) persistence
appendonly yes
appendfsync everysec

# Enable RDB snapshots
save 900 1
save 300 10
save 60 10000
```

**Benefits:**
- âœ… Redis keys survive restarts
- âœ… Reduces lock loss events
- âš ï¸ Trade-off: Slight performance impact

**Note:** Even with persistence, fail-fast is still needed (Redis can still crash/lose data)

---

### 4. **Graceful Shutdown** (Future Enhancement)

```java
// Instead of System.exit(1), use graceful shutdown
@Scheduled(fixedDelayString = "PT12H")
public void refreshTsidLock() {
    String currentValue = redisTemplate.opsForValue().get(key);
    
    if (currentValue == null || !currentValue.equals(instanceId)) {
        logger.error("CRITICAL: TSID lock lost or stolen - initiating graceful shutdown");
        
        // Graceful shutdown (allows in-flight requests to complete)
        applicationContext.close();
        return;
    }
    
    // Refresh TTL
    redisTemplate.expire(key, Duration.ofHours(24));
}
```

**Benefits:**
- âœ… Clean shutdown (no abrupt termination)
- âœ… In-flight requests complete
- âœ… Better for production

---

## ğŸ§ª Testing Auto-Restart

### Test 1: Docker Restart Policy

```bash
# 1. Start container with restart policy
docker run -d --name backend --restart=always silre-backend:latest

# 2. Simulate lock loss (delete Redis key)
redis-cli DEL "sys:tsid:node:0"

# 3. Wait for refresh (or trigger manually)
# Expected: Container restarts automatically

# 4. Check container status
docker ps -a | grep backend
# Expected: Container restarted, new container ID

# 5. Check logs
docker logs backend | tail -20
# Expected: "Re-acquired Node ID 0"
```

---

### Test 2: systemd Auto-Restart

```bash
# 1. Start service
sudo systemctl start silre-backend

# 2. Simulate lock loss
redis-cli DEL "sys:tsid:node:0"

# 3. Wait for refresh
# Expected: Service restarts automatically

# 4. Check service status
sudo systemctl status silre-backend
# Expected: "Active: active (running)" (after restart)

# 5. Check logs
sudo journalctl -u silre-backend -n 50
# Expected: "Re-acquired Node ID 0"
```

---

### Test 3: Kubernetes Auto-Restart

```bash
# 1. Deploy application
kubectl apply -f k8s/deployment.yaml

# 2. Simulate lock loss
kubectl exec -it redis-pod -- redis-cli DEL "sys:tsid:node:0"

# 3. Wait for refresh
# Expected: Pod restarts automatically

# 4. Check pod status
kubectl get pods -l app=silre-backend
# Expected: Pod restarted, new pod name

# 5. Check logs
kubectl logs -l app=silre-backend --tail=50
# Expected: "Re-acquired Node ID 0"
```

---

## ğŸ“ Summary

### âœ… **Production Setup Checklist:**

1. âœ… **Auto-restart mechanism** (systemd/Docker/K8s)
2. âœ… **Multiple instances** (high availability)
3. âœ… **Monitoring & alerting** (detect lock loss events)
4. âœ… **Redis persistence** (optional, reduces lock loss)
5. âœ… **Health checks** (liveness/readiness probes)

### âœ… **Self-Healing Flow:**

1. âœ… Lock lost â†’ Fail-fast (crash)
2. âœ… Auto-restart mechanism detects crash
3. âœ… Instance restarts
4. âœ… Re-acquires Node ID (same or different)
5. âœ… Back to normal operation

### âš ï¸ **Important Notes:**

1. âš ï¸ **Fail-fast is intentional** - prevents data corruption
2. âš ï¸ **Auto-restart is required** - ensures high availability
3. âš ï¸ **Multiple instances recommended** - zero downtime during restart
4. âš ï¸ **Monitoring is essential** - detect and alert on lock loss events

---

## ğŸ“ Best Practices

1. âœ… **Fail-Fast + Auto-Restart** = Safe + Available
2. âœ… **Multiple Instances** = Zero Downtime
3. âœ… **Monitoring** = Visibility into lock health
4. âœ… **Redis Persistence** = Reduces lock loss events
5. âœ… **Health Checks** = Fast failure detection

---

## âœ… Káº¿t luáº­n

**Trong production:**
- âœ… **Fail-fast** Ä‘á»ƒ prevent corruption (intentional crash)
- âœ… **Auto-restart** Ä‘á»ƒ ensure availability (systemd/Docker/K8s)
- âœ… **Self-healing** khi restart (re-acquire Node ID)
- âœ… **Multiple instances** Ä‘á»ƒ zero downtime

**KhÃ´ng cáº§n restart thá»§ cÃ´ng** - auto-restart mechanism sáº½ handle!
